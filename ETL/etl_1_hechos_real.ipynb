{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este archivo ipynb vamos a desmenuzar el xlsx original para poder trabajar con la solapa 'hechos':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los archivos auxiliares desde el xlsx para luego pasarlos a csv:\n",
    "df_accidentes = pd.read_excel('Datasets_originales/homicidios.xlsx', sheet_name='HECHOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para no confundirnos con las 'tablas de hechos', en vez de llamarle asi a este df le llamaremos 'accidentes'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidentes_copy = df_accidentes.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos todos los nombres de las columnas a mayúsculas:\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de las columnas sin acentos:\n",
      "Index(['ID', 'N_VICTIMAS', 'FECHA', 'AAAA', 'MM', 'DD', 'HORA', 'HH',\n",
      "       'LUGAR_DEL_HECHO', 'TIPO_DE_CALLE', 'CALLE', 'ALTURA', 'CRUCE',\n",
      "       'DIRECCION NORMALIZADA', 'COMUNA', 'XY (CABA)', 'POS X', 'POS Y',\n",
      "       'PARTICIPANTES', 'VICTIMA', 'ACUSADO'],\n",
      "      dtype='object')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sacamos los acentos a los nombres de las columnas:\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('Á', 'A')\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('É', 'E')\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('Í', 'I')\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('Ó', 'O')\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('Ú', 'U')\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace('Ñ', 'N')\n",
    "print('Nombres de las columnas sin acentos:')\n",
    "print(df_accidentes_copy.columns)\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de las columnas con _ en lugar de espacio:\n",
      "Index(['ID', 'N_VICTIMAS', 'FECHA', 'AAAA', 'MM', 'DD', 'HORA', 'HH',\n",
      "       'LUGAR_DEL_HECHO', 'TIPO_DE_CALLE', 'CALLE', 'ALTURA', 'CRUCE',\n",
      "       'DIRECCION_NORMALIZADA', 'COMUNA', 'XY_(CABA)', 'POS_X', 'POS_Y',\n",
      "       'PARTICIPANTES', 'VICTIMA', 'ACUSADO'],\n",
      "      dtype='object')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#coloco el _ en el medio de las palabras compuestas:\n",
    "df_accidentes_copy.columns = df_accidentes_copy.columns.str.replace(' ', '_')\n",
    "print('Nombres de las columnas con _ en lugar de espacio:')\n",
    "print(df_accidentes_copy.columns)\n",
    "print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar letras raros en todo el dataframe:\n",
    "\n",
    "# Crear un DataFrame vacío para almacenar los resultados\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for col in df_accidentes_copy.columns:\n",
    "    # Buscar valores no ASCII\n",
    "    non_ascii_values = df_accidentes_copy[col].apply(lambda x: re.search(r'[^\\x00-\\x7F]', str(x)) is not None)\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df = pd.DataFrame(df_accidentes_copy[col][non_ascii_values].unique(), columns=[col])\n",
    "    # Añadir los resultados al DataFrame de resultados\n",
    "    result = pd.concat([result, df], axis=1)\n",
    "\n",
    "# Guardar el DataFrame de resultados en un archivo CSV\n",
    "result.to_csv('non_ascii_values.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En el csv que nos devolvio, encontramos varios tipos de caracteres raros, casi todos relacionados con la ñ y Ñ vamos a comenzar por reemplazarlos:\n",
    "df_accidentes_copy['DIRECCION_NORMALIZADA'] = df_accidentes_copy['DIRECCION_NORMALIZADA'].str.replace('Ã‘', 'Ñ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el segundoo es Ã‰ que corresponde a É:\n",
    "df_accidentes_copy['DIRECCION_NORMALIZADA'] = df_accidentes_copy['DIRECCION_NORMALIZADA'].str.replace('Ã‰', 'É')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El tercero es Ã, que corresponde a Á:\n",
    "df_accidentes_copy['DIRECCION_NORMALIZADA'] = df_accidentes_copy['DIRECCION_NORMALIZADA'].str.replace('Ã', 'Á')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y el cuarto y ultimo es el Â° que en este caso y solo en este caso corresponde a ° A:\n",
    "df_accidentes_copy['DIRECCION_NORMALIZADA'] = df_accidentes_copy['DIRECCION_NORMALIZADA'].str.replace('Â°', '° A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Haciendo una una inspección visual de la columna 'DIRECCION_NORMALIZADA' encontramos que hay un valor que no corresponde a una dirección, sino a un nombre de calle, por lo que lo vamos a reemplazar, el mismo es NU?EZ por NUÑEZ:\n",
    "df_accidentes_copy['CALLE'] = df_accidentes_copy['CALLE'].str.replace('NU?EZ', 'NUÑEZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en el dataset de accidentes:\n",
      "ID                         0\n",
      "N_VICTIMAS                 0\n",
      "FECHA                      0\n",
      "AAAA                       0\n",
      "MM                         0\n",
      "DD                         0\n",
      "HORA                       0\n",
      "HH                         0\n",
      "LUGAR_DEL_HECHO            0\n",
      "TIPO_DE_CALLE              0\n",
      "CALLE                      1\n",
      "ALTURA                   567\n",
      "CRUCE                    171\n",
      "DIRECCION_NORMALIZADA      8\n",
      "COMUNA                     0\n",
      "XY_(CABA)                  0\n",
      "POS_X                      0\n",
      "POS_Y                      0\n",
      "PARTICIPANTES              0\n",
      "VICTIMA                    0\n",
      "ACUSADO                    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# buscamos los valores nulos en el dataset de accidentes:\n",
    "print('Valores nulos en el dataset de accidentes:')\n",
    "print(df_accidentes_copy.isnull().sum())\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a eliminar las columnas que tienen mas nulos que datos:\n",
    "# df_accidentes_copy = df_accidentes_copy.dropna(axis=1, thresh=0.5*len(df_accidentes))\n",
    "# print('Columnas eliminadas:')\n",
    "# print(df_accidentes_copy.isnull().sum())\n",
    "# print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas no eliminadas:\n",
      "Index(['ID', 'N_VICTIMAS', 'FECHA', 'AAAA', 'MM', 'DD', 'HORA', 'HH',\n",
      "       'LUGAR_DEL_HECHO', 'TIPO_DE_CALLE', 'CALLE', 'DIRECCION_NORMALIZADA',\n",
      "       'COMUNA', 'POS_X', 'POS_Y', 'PARTICIPANTES', 'VICTIMA', 'ACUSADO'],\n",
      "      dtype='object')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vamos a eliminar las columnas 'ALTURA', 'CRUCE' Y 'XY (CABA)' ya que no aportan informacion relevante:\n",
    "df_accidentes_copy = df_accidentes_copy.drop(columns=['ALTURA', 'CRUCE','XY_(CABA)'])\n",
    "print('Columnas no eliminadas:')\n",
    "print(df_accidentes_copy.columns)\n",
    "print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con el valor nulo en la columna CALLE:\n",
      "            ID  N_VICTIMAS      FECHA  AAAA  MM  DD      HORA  HH  \\\n",
      "119  2016-0151           1 2016-11-18  2016  11  18  20:35:00  20   \n",
      "\n",
      "    LUGAR_DEL_HECHO TIPO_DE_CALLE CALLE DIRECCION_NORMALIZADA  COMUNA POS_X  \\\n",
      "119              SD         CALLE   NaN                   NaN       0     .   \n",
      "\n",
      "    POS_Y PARTICIPANTES VICTIMA ACUSADO  \n",
      "119     .     PEATON-SD  PEATON      SD  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#buscar la fila con el valor nulo en la columna 'CALLE' y 'DIRECCION NORMALIZADA':\n",
    "print('Fila con el valor nulo en la columna CALLE:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['CALLE'].isnull()])\n",
    "print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila 119 completa:\n",
      "ID                                 2016-0151\n",
      "N_VICTIMAS                                 1\n",
      "FECHA                    2016-11-18 00:00:00\n",
      "AAAA                                    2016\n",
      "MM                                        11\n",
      "DD                                        18\n",
      "HORA                                20:35:00\n",
      "HH                                        20\n",
      "LUGAR_DEL_HECHO                           SD\n",
      "TIPO_DE_CALLE                          CALLE\n",
      "CALLE                                    NaN\n",
      "DIRECCION_NORMALIZADA                    NaN\n",
      "COMUNA                                     0\n",
      "POS_X                                      .\n",
      "POS_Y                                      .\n",
      "PARTICIPANTES                      PEATON-SD\n",
      "VICTIMA                               PEATON\n",
      "ACUSADO                                   SD\n",
      "Name: 119, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VER LA fila 119 completa:\n",
    "print('Fila 119 completa:')\n",
    "print(df_accidentes_copy.iloc[119])\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila 119 eliminada:\n",
      "ID                                          2016-0152\n",
      "N_VICTIMAS                                          1\n",
      "FECHA                             2016-11-23 00:00:00\n",
      "AAAA                                             2016\n",
      "MM                                                 11\n",
      "DD                                                 23\n",
      "HORA                                         21:10:00\n",
      "HH                                                 21\n",
      "LUGAR_DEL_HECHO            AV PERITO MORENO Y ZUVIRIA\n",
      "TIPO_DE_CALLE                                 AVENIDA\n",
      "CALLE                              MORENO, PERITO AV.\n",
      "DIRECCION_NORMALIZADA    MORENO, PERITO AV. y ZUVIRIA\n",
      "COMUNA                                              7\n",
      "POS_X                                    -58.46451841\n",
      "POS_Y                                    -34.64991946\n",
      "PARTICIPANTES                               MOTO-AUTO\n",
      "VICTIMA                                          MOTO\n",
      "ACUSADO                                          AUTO\n",
      "Name: 120, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BORRAMOS la fila 119 ya que carece de demasiada informacion relevante:\n",
    "df_accidentes_copy = df_accidentes_copy.drop([119])\n",
    "print('Fila 119 eliminada:')\n",
    "print(df_accidentes_copy.iloc[119])\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con el valor nulo en la columna DIRECCION NORMALIZADA:\n",
      "            ID  N_VICTIMAS      FECHA  AAAA  MM  DD      HORA  HH  \\\n",
      "38   2016-0052           1 2016-04-20  2016   4  20  20:00:00  20   \n",
      "106  2016-0136           1 2016-10-25  2016  10  25  00:00:00   0   \n",
      "180  2017-0050           2 2017-04-28  2017   4  28  11:08:08  11   \n",
      "181  2017-0051           1 2017-05-01  2017   5   1  03:47:47   3   \n",
      "313  2018-0039           1 2018-04-21  2018   4  21  22:15:00  22   \n",
      "546  2020-0026           1 2020-05-17  2020   5  17  06:40:00   6   \n",
      "621  2021-0023           1 2021-03-01  2021   3   1  09:20:00   9   \n",
      "\n",
      "                             LUGAR_DEL_HECHO TIPO_DE_CALLE  \\\n",
      "38                AUTOPISTA LUGONES PK 10000     AUTOPISTA   \n",
      "106         AU BUENOS AIRES - LA PLATA KM. 4     AUTOPISTA   \n",
      "180  AU PERITO MORENO Y RAMAL ENLACE AU1/AU6     AUTOPISTA   \n",
      "181                       AU DELLEPIANE 2400     AUTOPISTA   \n",
      "313                 AUTOPISTA LUGONES KM 4.7     AUTOPISTA   \n",
      "546             LUGONES, LEOPOLDO AV. KM 6,1     AUTOPISTA   \n",
      "621         AU BUENOS AIRES LA PLATA  KM 4,5     AUTOPISTA   \n",
      "\n",
      "                                    CALLE DIRECCION_NORMALIZADA  COMUNA POS_X  \\\n",
      "38                  LUGONES, LEOPOLDO AV.                   NaN      13     .   \n",
      "106     AUTOPISTA BUENOS AIRES - LA PLATA                   NaN       4     .   \n",
      "180               AUTOPISTA PERITO MORENO                   NaN       9     .   \n",
      "181  AUTOPISTA DELLEPIANE LUIS TTE. GRAL.                   NaN       7     .   \n",
      "313                 LUGONES, LEOPOLDO AV.                   NaN      14     .   \n",
      "546                 LUGONES, LEOPOLDO AV.                   NaN      14     .   \n",
      "621     AUTOPISTA BUENOS AIRES - LA PLATA                   NaN       4     .   \n",
      "\n",
      "    POS_Y     PARTICIPANTES VICTIMA      ACUSADO  \n",
      "38      .           MOTO-SD    MOTO           SD  \n",
      "106     .       MOTO-CARGAS    MOTO       CARGAS  \n",
      "180     .       MOTO-CARGAS    MOTO       CARGAS  \n",
      "181     .         AUTO-AUTO    AUTO         AUTO  \n",
      "313     .       PEATON-AUTO  PEATON         AUTO  \n",
      "546     .  MOTO-OBJETO FIJO    MOTO  OBJETO FIJO  \n",
      "621     .       MOTO-CARGAS    MOTO       CARGAS  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fila con el valor nulo en la columna DIRECCION NORMALIZADA:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['DIRECCION_NORMALIZADA'].isnull()])\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'N_VICTIMAS', 'FECHA', 'AAAA', 'MM', 'DD', 'HORA', 'HH',\n",
       "       'LUGAR_DEL_HECHO', 'TIPO_DE_CALLE', 'CALLE', 'DIRECCION_NORMALIZADA',\n",
       "       'COMUNA', 'POS_X', 'POS_Y', 'PARTICIPANTES', 'VICTIMA', 'ACUSADO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accidentes_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores NaN en DIRECCION_NORMALIZADA: 7\n"
     ]
    }
   ],
   "source": [
    "#este codigo verifica la cantidad de valores NaN en la columna DIRECCION_NORMALIZADA.\n",
    "#Reemplaza los valores NaN en DIRECCION_NORMALIZADA con los valores de LUGAR_DEL_HECHO.\n",
    "#Evalúa si DIRECCION_NORMALIZADA es redundante comparándola con LUGAR_DEL_HECHO. Si son iguales, elimina DIRECCION_NORMALIZADA.\n",
    "#Guarda el DataFrame limpio en un archivo CSV.\n",
    "\n",
    "# Verificar valores NaN en DIRECCION_NORMALIZADA\n",
    "nan_count = df_accidentes_copy['DIRECCION_NORMALIZADA'].isna().sum()\n",
    "print(f\"Valores NaN en DIRECCION_NORMALIZADA: {nan_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/f2bv9fls2wl6xxl27gbh6lnc0000gn/T/ipykernel_29177/3072453504.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_accidentes_copy['DIRECCION_NORMALIZADA'].fillna(df_accidentes_copy['LUGAR_DEL_HECHO'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar NaN en DIRECCION_NORMALIZADA con LUGAR_DEL_HECHO si no está vacío\n",
    "df_accidentes_copy['DIRECCION_NORMALIZADA'].fillna(df_accidentes_copy['LUGAR_DEL_HECHO'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECCION_NORMALIZADA contiene información única y debe ser conservada.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si DIRECCION_NORMALIZADA sigue siendo necesaria\n",
    "if df_accidentes_copy['DIRECCION_NORMALIZADA'].equals(df_accidentes_copy['LUGAR_DEL_HECHO']):\n",
    "    print(\"DIRECCION_NORMALIZADA es redundante y puede ser eliminada.\")\n",
    "    df_accidentes_copy.drop(columns=['DIRECCION_NORMALIZADA'], inplace=True)\n",
    "else:\n",
    "    print(\"DIRECCION_NORMALIZADA contiene información única y debe ser conservada.\")\n",
    "\n",
    "# Este código compara dos columnas del DataFrame df_accidentes_copy: 'DIRECCION_NORMALIZADA' y 'LUGAR_DEL_HECHO'.\n",
    "\n",
    "# El método equals() verifica si las dos columnas son idénticas, es decir, si todos sus elementos correspondientes son iguales.\n",
    "\n",
    "# Si las dos columnas son idénticas, el código imprime \"DIRECCION_NORMALIZADA es redundante y puede ser eliminada.\" y luego elimina la columna 'DIRECCION_NORMALIZADA' del DataFrame utilizando el método drop(). El argumento inplace=True significa que la operación se realiza en el DataFrame original, en lugar de devolver un nuevo DataFrame.\n",
    "\n",
    "# Si las dos columnas no son idénticas, el código imprime \"DIRECCION_NORMALIZADA contiene información única y debe ser conservada.\". Esto significa que la columna 'DIRECCION_NORMALIZADA' contiene al menos algún dato que no está presente en la columna 'LUGAR_DEL_HECHO', por lo que no se elimina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/f2bv9fls2wl6xxl27gbh6lnc0000gn/T/ipykernel_29177/2400321010.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mask = df_accidentes_copy.applymap(lambda x: 'SD' in str(x))\n"
     ]
    }
   ],
   "source": [
    "# BUSCAR todas las celdas que contengan la palabra 'SD'\n",
    "\n",
    "# Crear una máscara booleana donde True indica que la celda contiene 'SD'\n",
    "mask = df_accidentes_copy.applymap(lambda x: 'SD' in str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 18)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir las filas donde al menos una celda contiene 'SD'\n",
    "(df_accidentes_copy[mask.any(axis=1)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 695 entries, 0 to 695\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   ID                     695 non-null    object        \n",
      " 1   N_VICTIMAS             695 non-null    int64         \n",
      " 2   FECHA                  695 non-null    datetime64[ns]\n",
      " 3   AAAA                   695 non-null    int64         \n",
      " 4   MM                     695 non-null    int64         \n",
      " 5   DD                     695 non-null    int64         \n",
      " 6   HORA                   695 non-null    object        \n",
      " 7   HH                     695 non-null    object        \n",
      " 8   LUGAR_DEL_HECHO        695 non-null    object        \n",
      " 9   TIPO_DE_CALLE          695 non-null    object        \n",
      " 10  CALLE                  695 non-null    object        \n",
      " 11  DIRECCION_NORMALIZADA  695 non-null    object        \n",
      " 12  COMUNA                 695 non-null    int64         \n",
      " 13  POS_X                  695 non-null    object        \n",
      " 14  POS_Y                  695 non-null    object        \n",
      " 15  PARTICIPANTES          695 non-null    object        \n",
      " 16  VICTIMA                695 non-null    object        \n",
      " 17  ACUSADO                695 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(5), object(12)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_accidentes_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HORA', 'HH', 'PARTICIPANTES', 'VICTIMA', 'ACUSADO'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Crear una máscara booleana donde True indica que la celda contiene 'SD'\n",
    "mask = df_accidentes_copy.apply(lambda x: x.str.contains('SD').any() if x.dtype == \"object\" else False)\n",
    "\n",
    "# Imprimir las columnas donde al menos una celda contiene 'SD'\n",
    "print(df_accidentes_copy.columns[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con SD en la columna HORA:\n",
      "            ID  N_VICTIMAS      FECHA  AAAA  MM  DD HORA  HH  \\\n",
      "518  2019-0103           1 2019-12-18  2019  12  18   SD  SD   \n",
      "\n",
      "             LUGAR_DEL_HECHO TIPO_DE_CALLE           CALLE  \\\n",
      "518  PAZ, GRAL. AV. Y GRIVEO      GRAL PAZ  PAZ, GRAL. AV.   \n",
      "\n",
      "       DIRECCION_NORMALIZADA  COMUNA         POS_X         POS_Y  \\\n",
      "518  PAZ, GRAL. AV. y GRIVEO      11  -58.52169422  -34.59471640   \n",
      "\n",
      "    PARTICIPANTES VICTIMA ACUSADO  \n",
      "518     MOTO-MOTO    MOTO    MOTO  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostrar las filas que contienen SD en la columna 'HORA','HH','PARTICIPANTES','VICTIMA', 'ACUSADO'\n",
    "print('Fila con SD en la columna HORA:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['HORA'].str.contains('SD', na=False)])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la mediana de la columna 'HORA'\n",
    "\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Función para convertir un string a un tiempo\n",
    "def str_a_tiempo(s):\n",
    "    try:\n",
    "        return datetime.strptime(s, '%H:%M:%S').time()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Convertir los strings en la columna 'HORA' a objetos datetime.time\n",
    "df_accidentes_copy['HORA'] = df_accidentes_copy['HORA'].apply(lambda x: str_a_tiempo(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir los tiempos en la columna 'HORA' a segundos desde la medianoche\n",
    "segundos_desde_la_medianoche = df_accidentes_copy['HORA'].dropna().apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "\n",
    "# Calcular la mediana de los segundos\n",
    "mediana_segundos = segundos_desde_la_medianoche.median()\n",
    "\n",
    "# Convertir la mediana de segundos de vuelta a un tiempo\n",
    "mediana_HORA = time(int(mediana_segundos // 3600), int((mediana_segundos % 3600) // 60), int(mediana_segundos % 60))\n",
    "\n",
    "# Reemplazar los NaN en la columna 'HORA' con la mediana\n",
    "df_accidentes_copy['HORA'] = df_accidentes_copy['HORA'].fillna(mediana_HORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con SD en la columna HORA:\n",
      "Empty DataFrame\n",
      "Columns: [ID, N_VICTIMAS, FECHA, AAAA, MM, DD, HORA, HH, LUGAR_DEL_HECHO, TIPO_DE_CALLE, CALLE, DIRECCION_NORMALIZADA, COMUNA, POS_X, POS_Y, PARTICIPANTES, VICTIMA, ACUSADO]\n",
      "Index: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisamos si ha quedado SD en la columna 'HORA'\n",
    "print('Fila con SD en la columna HORA:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['HORA'].str.contains('SD', na=False)])\n",
    "print('\\n')\n",
    "\n",
    "#solucionado entonces el problema de SD en la columna 'HORA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/f2bv9fls2wl6xxl27gbh6lnc0000gn/T/ipykernel_29177/2442409241.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_accidentes_copy['HH'] = df_accidentes_copy['HH'].replace('SD', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Calcular la mediana de la columna 'HH'\n",
    "import numpy as np\n",
    "\n",
    "# Reemplazar 'SD' con NaN\n",
    "df_accidentes_copy['HH'] = df_accidentes_copy['HH'].replace('SD', np.nan)\n",
    "\n",
    "# Calcular la mediana de la columna 'HH'\n",
    "mediana_HH = df_accidentes_copy['HH'].astype(float).median()\n",
    "\n",
    "# Reemplazar los NaN en la columna 'HH' con la mediana\n",
    "df_accidentes_copy['HH'] = df_accidentes_copy['HH'].fillna(mediana_HH)\n",
    "\n",
    "# Convertir la columna 'HH' a int\n",
    "df_accidentes_copy['HH'] = df_accidentes_copy['HH'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con SD en la columna HH:\n",
      "Empty DataFrame\n",
      "Columns: [ID, N_VICTIMAS, FECHA, AAAA, MM, DD, HORA, HH, LUGAR_DEL_HECHO, TIPO_DE_CALLE, CALLE, DIRECCION_NORMALIZADA, COMUNA, POS_X, POS_Y, PARTICIPANTES, VICTIMA, ACUSADO]\n",
      "Index: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'HH' a string\n",
    "df_accidentes_copy['HH'] = df_accidentes_copy['HH'].astype(str)\n",
    "\n",
    "# Revisar si ha quedado 'SD' en la columna 'HH'\n",
    "print('Fila con SD en la columna HH:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['HH'].str.contains('SD', na=False)])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#volvemos a convertir la columna 'HH' a int\n",
    "df_accidentes_copy['HH'] = df_accidentes_copy['HH'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n"
     ]
    }
   ],
   "source": [
    "print(df_accidentes_copy[df_accidentes_copy['PARTICIPANTES'].str.contains('SD', na=False)]['PARTICIPANTES'].shape)#sin el shape nos muestra la columna completa\n",
    "# No la vamos a tomar como un problema ya que no es relevante para el análisis de los datos, ademas lo usaremos para analizar los Sin Datos en la columna 'VICTIMA' y 'ACUSADO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con SD en la columna VICTIMA:\n",
      "(9, 18)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fila con SD en la columna VICTIMA:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['VICTIMA'].str.contains('SD', na=False)].shape)#sin el shape nos muestra la columna completa\n",
    "print('\\n')\n",
    "\n",
    "# No la vamos a tomar como un problema ya que no es relevante para el análisis de los datos, ademas lo usaremos para analizar los Sin Datos en la columna 'PARTICIPANTES' y 'ACUSADO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila con SD en la columna ACUSADO:\n",
      "(22, 18)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fila con SD en la columna ACUSADO:')\n",
    "print(df_accidentes_copy[df_accidentes_copy['ACUSADO'].str.contains('SD', na=False)].shape)#sin el shape nos muestra la columna completa\n",
    "print('\\n')\n",
    "\n",
    "# No la vamos a tomar como un problema ya que no es relevante para el análisis de los datos, ademas lo usaremos para analizar los Sin Datos en la columna 'PARTICIPANTES' y 'VICTIMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en el dataset de accidentes:\n",
      "ID                       0\n",
      "N_VICTIMAS               0\n",
      "FECHA                    0\n",
      "AAAA                     0\n",
      "MM                       0\n",
      "DD                       0\n",
      "HORA                     0\n",
      "HH                       0\n",
      "LUGAR_DEL_HECHO          0\n",
      "TIPO_DE_CALLE            0\n",
      "CALLE                    0\n",
      "DIRECCION_NORMALIZADA    0\n",
      "COMUNA                   0\n",
      "POS_X                    0\n",
      "POS_Y                    0\n",
      "PARTICIPANTES            0\n",
      "VICTIMA                  0\n",
      "ACUSADO                  0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisar nuevamente si han quedado nulls en el dataset de accidentes:\n",
    "print('Valores nulos en el dataset de accidentes:')\n",
    "print(df_accidentes_copy.isnull().sum())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con todo el DF limpio y ordenado, agregamos la columna 'FRANJA_HORARIA' para luego hacer ahalisis por rangos horarios:\n",
    "#NOTA IMPORTANTE: en los 'bins' se debe colocar -1 y25 como limites para que los datos se ajusten correctamente, ya que si se coloca 0 y 24, los datos no se ajustan correctamente, y el resultado es un Nan.-\n",
    "\n",
    "df_accidentes_copy['FRANJA_HORARIA'] = pd.cut(df_accidentes_copy['HH'], bins=[-1, 6, 12, 18, 25], labels=['Madrugada', 'Mañana', 'Tarde', 'Noche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en el dataset de accidentes:\n",
      "ID                       0\n",
      "N_VICTIMAS               0\n",
      "FECHA                    0\n",
      "AAAA                     0\n",
      "MM                       0\n",
      "DD                       0\n",
      "HORA                     0\n",
      "HH                       0\n",
      "LUGAR_DEL_HECHO          0\n",
      "TIPO_DE_CALLE            0\n",
      "CALLE                    0\n",
      "DIRECCION_NORMALIZADA    0\n",
      "COMUNA                   0\n",
      "POS_X                    0\n",
      "POS_Y                    0\n",
      "PARTICIPANTES            0\n",
      "VICTIMA                  0\n",
      "ACUSADO                  0\n",
      "FRANJA_HORARIA           0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#revisamos nuevamente si han quedado nulls en el dataset de accidentes:\n",
    "print('Valores nulos en el dataset de accidentes:')\n",
    "print(df_accidentes_copy.isnull().sum())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVEMOS la columna 'FRANJA_HORARIA' al lado de 'HH' para que quede mas ordenado:\n",
    "\n",
    "# Extraer la columna 'FRANJA_HORARIA'\n",
    "franja_horaria = df_accidentes_copy.pop('FRANJA_HORARIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar la columna 'FRANJA_HORARIA' en la posición deseada\n",
    "df_accidentes_copy.insert(df_accidentes_copy.columns.get_loc('HH') + 1, 'FRANJA_HORARIA', franja_horaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el DataFrame limpio listo para usar en un archivo xlsx:\n",
    "df_accidentes_copy.to_excel('Datasets_limpios/df_hechos_ETL.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el DataFrame limpio listo para usar en un archivo CSV:\n",
    "df_accidentes_copy.to_csv('Datasets_limpios/df_hechos_ETL.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
